# awesome-llm-papers-interpretability (after 2020)
Focusing on: interpretability of large language models (LLM).

## survey
A Comprehensive Overview of Large Language Models. \[[pdf](https://arxiv.org/pdf/2307.06435.pdf)\]  \[2023.12\]

A Survey of Large Language Models. \[[pdf](https://arxiv.org/pdf/2303.18223.pdf)\]  \[2023.11\]

Explainability for Large Language Models: A Survey. \[[pdf](https://arxiv.org/pdf/2309.01029.pdf)\]  \[2023.11\]

Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. \[[pdf]
(https://arxiv.org/pdf/2207.13243.pdf)\]  \[2023.8\]

A Survey on In-context Learning. \[[pdf](https://arxiv.org/pdf/2301.00234.pdf)\]  \[2023.6\]

Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. \[[pdf](https://arxiv.org/pdf/2303.15647.pdf)\]  \[2023.3\]


## papers
Do Machine Learning Models Memorize or Generalize? \[[pdf](https://pair.withgoogle.com/explorables/grokking/)\] \[2023.8\]

Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. \[[pdf](https://arxiv.org/pdf/2305.14160.pdf)\] \[EMNLP 2023\] \[2023.5\]

What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. \[[pdf](https://arxiv.org/pdf/2305.09731.pdf)\] \[ACL 2023\] \[2023.5\]

Language models can explain neurons in language models. \[[pdf](https://openai.com/research/language-models-can-explain-neurons-in-language-models)\] \[2023.5\]

Dissecting Recall of Factual Associations in Auto-Regressive Language Models. \[[pdf](https://arxiv.org/pdf/2304.14767.pdf)\] \[EMNLP 2023\] \[2023.4\]

The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. \[[pdf](https://arxiv.org/pdf/2304.13276.pdf)\] \[ICLR 2024\] \[2023.4\]

How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. \[[pdf](https://arxiv.org/pdf/2305.00586.pdf)\] \[NeurIPS 2023\] \[2023.4\]

A Theory of Emergent In-Context Learning as Implicit Structure Induction. \[[pdf](https://arxiv.org/pdf/2303.07971.pdf)\] \[arxiv\] \[2023.3\]

Larger language models do in-context learning differently. \[[pdf](https://arxiv.org/pdf/2303.03846.pdf)\] \[ICLR 2024\] \[2023.3\]

Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. \[[pdf](https://arxiv.org/pdf/2301.04213.pdf)\] \[NeurIPs 2023\] \[2023.1\]

Transformers as Algorithms: Generalization and Stability in In-context Learning. \[[pdf](https://arxiv.org/pdf/2301.07067.pdf)\] \[ICML 2023\] \[2023.1\]

Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. \[[pdf](https://arxiv.org/pdf/2212.10559.pdf)\] \[ACL 2023\] \[2022.12\]

Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. \[[pdf](https://arxiv.org/pdf/2211.00593.pdf)\] \[ICLR 2023\] \[2022.11\]

What learning algorithm is in-context learning? Investigations with linear models. \[[pdf](https://arxiv.org/pdf/2211.15661.pdf)\] \[ICLR 2023\] \[2022.11\]

Mass-Editing Memory in a Transformer. \[[pdf](https://arxiv.org/pdf/2210.07229.pdf)\] \[ICLR 2023\] \[2022.10\]

Polysemanticity and Capacity in Neural Networks. \[[pdf](https://arxiv.org/pdf/2210.01892.pdf)\] \[2022.10\]

Analyzing Transformers in Embedding Space. \[[pdf](https://arxiv.org/pdf/2209.02535.pdf)\] \[ACL 2023\] \[2022.9\]

Toy Models of Superposition. \[[pdf](https://transformer-circuits.pub/2022/toy_model/index.html)\] \[2022.9\]

Emergent Abilities of Large Language Models. \[[pdf](https://arxiv.org/pdf/2206.07682.pdf)\] \[2022.6\]

Towards Tracing Factual Knowledge in Language Models Back to the Training Data. \[[pdf](https://arxiv.org/pdf/2205.11482.pdf)\] \[EMNLP 2022\] \[2022.5\]

Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. \[[pdf](https://arxiv.org/pdf/2205.12685.pdf)\] \[EMNLP 2022\] \[2022.5\]

Scaling Laws and Interpretability of Learning from Repeated Data. \[[pdf](https://arxiv.org/pdf/2205.10487.pdf)\] \[2022.5\]

Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. \[[pdf](https://arxiv.org/pdf/2203.14680.pdf)\] \[EMNLP 2022\] \[2022.3\]

In-context Learning and Induction Heads. \[[pdf](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\] \[2022.3\]

Locating and Editing Factual Associations in GPT. \[[pdf](https://arxiv.org/pdf/2202.05262.pdf)\] \[NeurIPS 2022\] \[2022.2\]

Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? \[[pdf](https://arxiv.org/pdf/2202.12837.pdf)\] \[EMNLP 2022\] \[2022.2\]

Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. \[[pdf](https://arxiv.org/pdf/2201.02177.pdf)\] \[2022.1\]

A Mathematical Framework for Transformer Circuits. \[[pdf](https://transformer-circuits.pub/2021/framework/index.html)\] \[2021.12\]

An Explanation of In-context Learning as Implicit Bayesian Inference. \[[pdf](https://arxiv.org/pdf/2104.08696.pdf)\] \[ICLR 2022\] \[2021.11\]

Do Prompt-Based Models Really Understand the Meaning of their Prompts? \[[pdf](https://arxiv.org/pdf/2109.01247.pdf)\] \[NAACL 2022\] \[2021.9\]

Deduplicating Training Data Makes Language Models Better. \[[pdf](https://arxiv.org/pdf/2107.06499.pdf)\] \[ACL 2022\] \[2021.7\]

Knowledge Neurons in Pretrained Transformers. \[[pdf](https://arxiv.org/pdf/2104.08696.pdf)\] \[ACL 2022\] \[2021.4\]

Editing Factual Knowledge in Language Models. \[[pdf](https://arxiv.org/pdf/2104.08164.pdf)\] \[EMNLP 2021\] \[2021.4\]

Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. \[[pdf](https://arxiv.org/pdf/2104.08786.pdf)\] \[ACL 2022\] \[2021.4\]
Transformer Feed-Forward Layers Are Key-Value Memories. \[[pdf](https://arxiv.org/pdf/2012.14913.pdf)\] \[EMNLP 2021\] \[2020.12\]
Scaling Laws for Neural Language Models. \[[pdf](https://arxiv.org/pdf/2001.08361.pdf)\] \[2020.11\]
